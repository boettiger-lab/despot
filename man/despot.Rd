% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/despot.R, R/solver.R
\name{despot}
\alias{despot}
\alias{solver}
\alias{despot}
\alias{DESPOT}
\title{despot}
\usage{
despot(transition, observation, reward, discount, state_prior = rep(1,
  dim(observation)[[1]])/dim(observation)[[1]], verbose = TRUE,
  log_dir = tempdir(), log_data = NULL, ...)

solver(model, output = tempfile(), runs = 2, stdout = tempfile(),
  stderr = tempfile(), timeout = NULL, simlen = NULL,
  max_policy_simlen = NULL, depth = NULL, discount = NULL)
}
\arguments{
\item{transition}{Transition matrix, dimension n_s x n_s x n_a}

\item{observation}{Observation matrix, dimension n_s x n_z x n_a}

\item{reward}{reward matrix, dimension n_s x n_a}

\item{discount}{the discount factor}

\item{state_prior}{initial belief state, optional, defaults to uniform over states}

\item{verbose}{logical, should the function include a message with pomdp diagnostics (timings, final precision, end condition)}

\item{log_dir}{pomdpx and simulation csv files will be saved here, along with a metadata file}

\item{...}{additional arguments to \code{\link{appl}}.}

\item{model}{file/path to the \code{pomdp} model file}

\item{stdout}{a filename where pomdp run data will be stored}

\item{stderr}{where output to 'stderr', see \code{\link{system2}}. Use \code{FALSE}
to suppress output.}

\item{simlen}{despot number of steps to simulate (default 90)}

\item{depth}{maximum depth to simulate (default 90)}

\item{timout}{despot search time per move, in seconds (default 1)}

\item{max-policy-simlen}{number of steps to simulate the default policy (default 90)}

\item{discount}{discount factor for the POMDP model (default from the model file)}
}
\value{
a matrix of alpha vectors. Column index indicates action associated with the alpha vector, (1:n_actions),
 rows indicate system state, x. Actions for which no alpha vector was found are included as all -Inf, since such actions are
 not optimal regardless of belief, and thus have no corresponding alpha vectors in alpha_action list.
}
\description{
despot wraps the tasks of writing the pomdpx file defining the problem, running the DESPOT algorithm in C++,
and then reading the resulting simulation file back into R.

Wrappers for the DESPOT executables. The \code{solver} function solves a model
file and returns the path to the output policy file.
}
\examples{
\dontrun{ ## Takes > 5s
## Use example code to generate matrices for pomdp problem:
source(system.file("examples/fisheries-ex.R", package = "despot"))
alpha <- despot(transition, observation, reward, discount, precision = 10)
compute_policy(alpha, transition, observation, reward)
}

\donttest{
model <- system.file("models/example.pomdp", package = "despot")
policy <- tempfile()
solver(model, output = policy, timeout = 1)

# Other tools
evaluation <- pomdpeval(model, policy, stdout = FALSE)
graph <- polgraph(model, policy, stdout = FALSE)
simulations <- pomdpsim(model, policy, stdout = FALSE)
}
}
